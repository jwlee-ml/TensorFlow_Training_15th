{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "5_2.RNN_Language_Model.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "S6hacssxFZqn",
        "colab_type": "code",
        "outputId": "e4001564-b4b2-43dc-c099-7d8e8a8fd3b8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "## library import\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "\n",
        "print(tf.__version__)\n",
        "print(keras.__version__)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.2.0-rc2\n",
            "2.3.0-tf\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J9JVGqJWyImj",
        "colab_type": "text"
      },
      "source": [
        "# 순환 신경망을 활용한 문자열 생성"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "36mY58LdyckL",
        "colab_type": "text"
      },
      "source": [
        "- forked from https://www.tensorflow.org/beta/tutorials/text/text_generation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4k_1RxmUycfW",
        "colab_type": "text"
      },
      "source": [
        "Note: 이 문서는 텐서플로 커뮤니티에서 번역했습니다. 커뮤니티 번역 활동의 특성상 정확한 번역과 최신 내용을 반영하기 위해 노력함에도 불구하고 [공식 영문 문서](https://github.com/tensorflow/docs/blob/r2.0rc/site/en/r2/tutorials/text/text_generation.ipynb)의 내용과 일치하지 않을 수 있습니다. 이 번역에 개선할 부분이 있다면 [tensorflow/docs](https://github.com/tensorflow/docs) 깃헙 저장소로 풀 리퀘스트를 보내주시기 바랍니다. 문서 번역이나 리뷰에 참여하려면 [docs-ko@tensorflow.org](https://groups.google.com/a/tensorflow.org/forum/#!forum/docs-ko)로 메일을 보내주시기 바랍니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vIB1N7rnycdE",
        "colab_type": "text"
      },
      "source": [
        "이 튜토리얼에서는 문자 기반 순환 신경망(RNN, Recurrent Neural Network)을 사용하여 어떻게 텍스트를 생성하는지 설명합니다. Andrej Karpathy의 [순환 신경망의 뛰어난 효율](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)에서 가져온 셰익스피어 데이터셋으로 작업할 것입니다. 이 데이터 셋에서 문자 시퀀스 (\"Shakespear\")가 주어지면, 시퀀스의 다음 문자(\"e\")를 예측하는 모델을 훈련합니다. 모델을 반복하여 호출하면 더 긴 텍스트 시퀀스 생성이 가능합니다.\n",
        "\n",
        "Note: 이 노트북을 더 빠르게 실행하기 위해 GPU 가속을 활성화합니다. 코랩(Colab)에서: Runtime > Change runtime type > Hardware acclerator > GPU* 탭을 선택합니다. 로컬에서 실행하려면 TensorFlow 버전이 1.11 이상이어야 합니다.\n",
        "\n",
        "이 튜토리얼은 [tf.keras](https://www.tensorflow.org/programmers_guide/keras)와 [즉시 실행(eager execution)](https://www.tensorflow.org/programmers_guide/eager)을 활용하여 구현된 실행 가능한 코드가 포함되어 있습니다. 다음은 이 튜토리얼의 30번의 에포크(Epoch)로 훈련된 모델에서 \"Q\" 문자열로 시작될 때의 샘플 출력입니다.\n",
        "\n",
        "<pre>\n",
        "QUEENE:\n",
        "I had thought thou hadst a Roman; for the oracle,\n",
        "Thus by All bids the man against the word,\n",
        "Which are so weak of care, by old care done;\n",
        "Your children were in your holy love,\n",
        "And the precipitation through the bleeding throne.\n",
        "\n",
        "BISHOP OF ELY:\n",
        "Marry, and will, my lord, to weep in such a one were prettiest;\n",
        "Yet now I was adopted heir\n",
        "Of the world's lamentable day,\n",
        "To watch the next way with his father with his face?\n",
        "\n",
        "ESCALUS:\n",
        "The cause why then we are all resolved more sons.\n",
        "\n",
        "VOLUMNIA:\n",
        "O, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, it is no sin it should be dead,\n",
        "And love and pale as any will to that word.\n",
        "\n",
        "QUEEN ELIZABETH:\n",
        "But how long have I heard the soul for this world,\n",
        "And show his hands of life be proved to stand.\n",
        "\n",
        "PETRUCHIO:\n",
        "I say he look'd on, if I must be content\n",
        "To stay him from the fatal of our country's bliss.\n",
        "His lordship pluck'd from this sentence then for prey,\n",
        "And then let us twain, being the moon,\n",
        "were she such a case as fills m\n",
        "</pre>\n",
        "\n",
        "문장 중 일부는 문법적으로 맞지만 대부분 자연스럽지 않습니다. 이 모델은 단어의 의미를 학습하지는 않았지만, 고려해야 할 점으로:\n",
        "\n",
        "* 모델은 문자 기반입니다. 훈련이 시작되었을 때, 이 모델은 영어 단어의 철자를 모르거나 심지어 텍스트의 단위가 단어라는 것도 모릅니다.\n",
        "\n",
        "* 출력의 구조는 대본과 유사합니다. 즉, 텍스트 블록은 대개 화자의 이름으로 시작하고 이 이름들은 모든 데이터셋에서 대문자로 씌여 있습니다.\n",
        "\n",
        "* 아래에 설명된 것처럼 이 모델은 작은 텍스트 배치(각 100자)로 훈련되었으며 논리적인 구조를 가진 더 긴 텍스트 시퀀스를 생성할 수 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rqLHYN7KyV6V",
        "colab_type": "code",
        "outputId": "8f7c02d8-d90d-4be7-a197-da78b71aae6e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "## Data 준비\n",
        "path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt\n",
            "1122304/1115394 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sDKovh8xyt9c",
        "colab_type": "code",
        "outputId": "1e737d23-e6cb-4f48-baaa-0e18299bb387",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        "# 읽은 다음 파이썬 2와 호환되도록 디코딩합니다.\n",
        "text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
        "# 텍스트의 길이는 그 안에 있는 문자의 수입니다.\n",
        "print ('텍스트의 길이: {}자'.format(len(text)))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "텍스트의 길이: 1115394자\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4CfwQ4zDy0HJ",
        "colab_type": "code",
        "outputId": "08da78e7-f2e6-4afc-978b-e912021909e3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        }
      },
      "source": [
        "# 텍스트의 처음 250자를 살펴봅니다\n",
        "print(text[:250])"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you know Caius Marcius is chief enemy to the people.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rbxb6L9Cy2B9",
        "colab_type": "code",
        "outputId": "17da2b82-32f9-400d-a353-4693dc3543a1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        "# 파일의 고유 문자수를 출력합니다.\n",
        "vocab = sorted(set(text))\n",
        "print ('고유 문자수 {}개'.format(len(vocab)))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "고유 문자수 65개\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ycedi0w8y6ct",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 고유 문자에서 인덱스로 매핑 생성\n",
        "char2idx = {u:i for i, u in enumerate(vocab)}\n",
        "idx2char = np.array(vocab)\n",
        "\n",
        "text_as_int = np.array([char2idx[c] for c in text])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hep0SiIRy-Gz",
        "colab_type": "code",
        "outputId": "258ef209-a994-45f7-9099-fa291d745062",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 446
        }
      },
      "source": [
        "print('{')\n",
        "for char,_ in zip(char2idx, range(20)):\n",
        "    print('  {:4s}: {:3d},'.format(repr(char), char2idx[char]))\n",
        "print('  ...\\n}')"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{\n",
            "  '\\n':   0,\n",
            "  ' ' :   1,\n",
            "  '!' :   2,\n",
            "  '$' :   3,\n",
            "  '&' :   4,\n",
            "  \"'\" :   5,\n",
            "  ',' :   6,\n",
            "  '-' :   7,\n",
            "  '.' :   8,\n",
            "  '3' :   9,\n",
            "  ':' :  10,\n",
            "  ';' :  11,\n",
            "  '?' :  12,\n",
            "  'A' :  13,\n",
            "  'B' :  14,\n",
            "  'C' :  15,\n",
            "  'D' :  16,\n",
            "  'E' :  17,\n",
            "  'F' :  18,\n",
            "  'G' :  19,\n",
            "  ...\n",
            "}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OuUFzGgNzAq7",
        "colab_type": "code",
        "outputId": "7b781138-5b76-43fb-df8e-450a4b4c8721",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        "# 텍스트에서 처음 13개의 문자가 숫자로 어떻게 매핑되었는지를 보여줍니다\n",
        "print ('{} ---- 문자들이 다음의 정수로 매핑되었습니다 ---- > {}'.format(repr(text[:13]), text_as_int[:13]))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "'First Citizen' ---- 문자들이 다음의 정수로 매핑되었습니다 ---- > [18 47 56 57 58  1 15 47 58 47 64 43 52]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JW2lzbZRzK1B",
        "colab_type": "text"
      },
      "source": [
        "### 예측 과정\n",
        "\n",
        "주어진 문자나 문자 시퀀스가 주어졌을 때, 다음 문자로 가장 가능성 있는 문자는 무엇일까요? 이는 모델을 훈련하여 수행할 작업입니다. 모델의 입력은 문자열 시퀀스가 될 것이고, 모델을 훈련시켜 출력을 예측합니다. 이 출력은 현재 타임 스텝(time step)의 다음 문자입니다.\n",
        "\n",
        "RNN은 이전에 본 요소에 의존하는 내부 상태를 유지하고 있으므로, 이 순간까지 계산된 모든 문자를 감안할 때, 다음 문자는 무엇일까요?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CwdoQRtIzLBG",
        "colab_type": "text"
      },
      "source": [
        "### 훈련 샘플과 타깃 만들기\n",
        "\n",
        "다음으로 텍스트를 샘플 시퀀스로 나눕니다. 각 입력 시퀀스에는 텍스트에서 나온 `seq_length`개의 문자가 포함될 것입니다.\n",
        "\n",
        "각 입력 시퀀스에서, 해당 타깃은 한 문자를 오른쪽으로 이동한 것을 제외하고는 동일한 길이의 텍스트를 포함합니다.\n",
        "\n",
        "따라서 텍스트를`seq_length + 1`개의 청크(chunk)로 나눕니다. 예를 들어, `seq_length`는 4이고 텍스트를 \"Hello\"이라고 가정해 봅시다. 입력 시퀀스는 \"Hell\"이고 타깃 시퀀스는 \"ello\"가 됩니다.\n",
        "\n",
        "이렇게 하기 위해 먼저 `tf.data.Dataset.from_tensor_slices` 함수를 사용해 텍스트 벡터를 문자 인덱스의 스트림으로 변환합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4qDWyXTezFoK",
        "colab_type": "code",
        "outputId": "854c5e2d-df05-4df7-d7e1-5b0d09837b73",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 110
        }
      },
      "source": [
        "# 단일 입력에 대해 원하는 문장의 최대 길이\n",
        "seq_length = 100\n",
        "examples_per_epoch = len(text)//seq_length\n",
        "\n",
        "# 훈련 샘플/타깃 만들기\n",
        "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
        "\n",
        "for i in char_dataset.take(5):\n",
        "  print(idx2char[i.numpy()])"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "F\n",
            "i\n",
            "r\n",
            "s\n",
            "t\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BZ3CflQGzT4x",
        "colab_type": "code",
        "outputId": "26162f5c-d25b-4944-814f-f68da54a1e15",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 110
        }
      },
      "source": [
        "sequences = char_dataset.batch(seq_length+1, drop_remainder=True)\n",
        "\n",
        "for item in sequences.take(5):\n",
        "  print(repr(''.join(idx2char[item.numpy()])))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n",
            "'are all resolved rather to die than to famish?\\n\\nAll:\\nResolved. resolved.\\n\\nFirst Citizen:\\nFirst, you k'\n",
            "\"now Caius Marcius is chief enemy to the people.\\n\\nAll:\\nWe know't, we know't.\\n\\nFirst Citizen:\\nLet us ki\"\n",
            "\"ll him, and we'll have corn at our own price.\\nIs't a verdict?\\n\\nAll:\\nNo more talking on't; let it be d\"\n",
            "'one: away, away!\\n\\nSecond Citizen:\\nOne word, good citizens.\\n\\nFirst Citizen:\\nWe are accounted poor citi'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6PcxS5RJzfMy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def split_input_target(chunk):\n",
        "    input_text = chunk[:-1]\n",
        "    target_text = chunk[1:]\n",
        "    return input_text, target_text\n",
        "\n",
        "dataset = sequences.map(split_input_target)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5HDGf2H1zmMI",
        "colab_type": "code",
        "outputId": "808f8c68-c8ac-40dd-f880-ddedb6a050a9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "## 첫 번째 data와 label 확인\n",
        "for input_example, target_example in  dataset.take(1):\n",
        "  print ('입력 데이터: ', repr(''.join(idx2char[input_example.numpy()])))\n",
        "  print ('타깃 데이터: ', repr(''.join(idx2char[target_example.numpy()])))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "입력 데이터:  'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou'\n",
            "타깃 데이터:  'irst Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mCFsO-UZztXg",
        "colab_type": "code",
        "outputId": "9c36c865-a0f4-434a-db26-2ef674203776",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        }
      },
      "source": [
        "for i, (input_idx, target_idx) in enumerate(zip(input_example[:5], target_example[:5])):\n",
        "    print(\"{:4d}단계\".format(i))\n",
        "    print(\"  입력: {} ({:s})\".format(input_idx, repr(idx2char[input_idx])))\n",
        "    print(\"  예상 출력: {} ({:s})\".format(target_idx, repr(idx2char[target_idx])))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "   0단계\n",
            "  입력: 18 ('F')\n",
            "  예상 출력: 47 ('i')\n",
            "   1단계\n",
            "  입력: 47 ('i')\n",
            "  예상 출력: 56 ('r')\n",
            "   2단계\n",
            "  입력: 56 ('r')\n",
            "  예상 출력: 57 ('s')\n",
            "   3단계\n",
            "  입력: 57 ('s')\n",
            "  예상 출력: 58 ('t')\n",
            "   4단계\n",
            "  입력: 58 ('t')\n",
            "  예상 출력: 1 (' ')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gbhnPNCvzzSz",
        "colab_type": "code",
        "outputId": "99d50b82-94cf-4b66-e3f9-886c6c96f96a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        "# 배치 크기\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "# 데이터셋을 섞을 버퍼 크기\n",
        "# (TF 데이터는 무한한 시퀀스와 함께 작동이 가능하도록 설계되었으며,\n",
        "# 따라서 전체 시퀀스를 메모리에 섞지 않습니다. 대신에,\n",
        "# 요소를 섞는 버퍼를 유지합니다).\n",
        "BUFFER_SIZE = 10000\n",
        "\n",
        "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
        "\n",
        "dataset"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<BatchDataset shapes: ((64, 100), (64, 100)), types: (tf.int64, tf.int64)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ih71_rfz8sy",
        "colab_type": "text"
      },
      "source": [
        "## 모델 설계\n",
        "\n",
        "모델을 정의하려면 `tf.keras.Sequential`을 사용합니다. 이 간단한 예제에서는 3개의 층을 사용하여 모델을 정의합니다:\n",
        "\n",
        "* `tf.keras.layers.Embedding` : 입력층. `embedding_dim` 차원 벡터에 각 문자의 정수 코드를 매핑하는 훈련 가능한 검색 테이블.\n",
        "* `tf.keras.layers.GRU` : 크기가 `units = rnn_units`인 RNN의 유형(여기서 LSTM층을 사용할 수도 있습니다.)\n",
        "* `tf.keras.layers.Dense` : 크기가 `vocab_size`인 출력을 생성하는 출력층."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h3ANOx36z25I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 문자로 된 어휘 사전의 크기\n",
        "vocab_size = len(vocab)\n",
        "\n",
        "# 임베딩 차원\n",
        "embedding_dim = 256\n",
        "\n",
        "# RNN 유닛(unit) 개수\n",
        "rnn_units = 1024"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k6JM4Q3G0Bf5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
        "  model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(vocab_size, embedding_dim,\n",
        "                              batch_input_shape=[batch_size, None]),\n",
        "    tf.keras.layers.LSTM(rnn_units,\n",
        "                        return_sequences=True,\n",
        "                        stateful=True,\n",
        "                        recurrent_initializer='glorot_uniform'),\n",
        "    tf.keras.layers.Dense(vocab_size)\n",
        "  ])\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pS8sMJdj0DIb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = build_model(\n",
        "  vocab_size = len(vocab),\n",
        "  embedding_dim=embedding_dim,\n",
        "  rnn_units=rnn_units,\n",
        "  batch_size=BATCH_SIZE)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xd03Yx8x0Eu4",
        "colab_type": "code",
        "outputId": "1adba48d-de82-4fe3-caf2-72ef2d752bb8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        "for input_example_batch, target_example_batch in dataset.take(1):\n",
        "  example_batch_predictions = model(input_example_batch)\n",
        "  print(example_batch_predictions.shape, \"# (배치 크기, 시퀀스 길이, 어휘 사전 크기)\")"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(64, 100, 65) # (배치 크기, 시퀀스 길이, 어휘 사전 크기)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N6JUI6ml0N06",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''모델로부터 실제 예측을 얻으려면 출력 배열에서 샘플링하여 실제 문자 인덱스를 얻어야 합니다. 이 분포는 문자 어휘에 대한 로짓에 의해 정의됩니다.\n",
        "\n",
        "Note: 배열에 argmax를 취하면 모델이 쉽게 루프에 걸릴 수 있으므로 배열에서 샘플링하는 것이 중요합니다.\n",
        "\n",
        "배치의 첫 번째 샘플링을 시도해 봅시다:'''\n",
        "\n",
        "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
        "sampled_indices = tf.squeeze(sampled_indices,axis=-1).numpy()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X7ytrTNF5z4j",
        "colab_type": "code",
        "outputId": "b121cfb8-1aea-4542-cdd6-e67c47dd0d22",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 129
        }
      },
      "source": [
        "'''이렇게 하면 각 타임 스텝(time step)에서 다음 문자 인덱스에 대한 예측을 제공합니다:'''\n",
        "\n",
        "sampled_indices"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([41, 60, 58, 22, 23, 46, 26,  5, 54, 37, 52, 24, 43, 32,  0, 50, 52,\n",
              "       42, 47, 36, 18, 29, 44, 39, 48, 39, 14, 62,  6, 39, 16, 14, 22, 30,\n",
              "       60, 61, 62,  4, 52,  3, 56, 54, 30, 19, 39, 35, 62, 13, 16, 24, 34,\n",
              "       13,  1, 14, 40, 29, 58, 47, 22,  0, 36, 29, 49, 64, 39, 41,  2, 11,\n",
              "       64, 42, 19, 52, 16, 41, 58, 12, 19, 19, 47, 16, 20, 57, 12, 59, 33,\n",
              "       44, 17, 15, 55, 56, 51, 35, 37,  2, 54, 32, 30, 36, 51, 56])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4wUdNq8g51pu",
        "colab_type": "code",
        "outputId": "831ff782-7f1c-4fe9-9b18-c25d5544ac25",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 110
        }
      },
      "source": [
        "'''훈련되지 않은 모델에 의해 예측된 텍스트를 보기 위해 복호화합니다.'''\n",
        "\n",
        "print(\"입력: \\n\", repr(\"\".join(idx2char[input_example_batch[0]])))\n",
        "print()\n",
        "print(\"예측된 다음 문자: \\n\", repr(\"\".join(idx2char[sampled_indices ])))"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "입력: \n",
            " 'me:\\nAnd tell me, then, have you not broke your oaths?\\n\\nFirst Keeper:\\nNo;\\nFor we were subjects but wh'\n",
            "\n",
            "예측된 다음 문자: \n",
            " \"cvtJKhN'pYnLeT\\nlndiXFQfajaBx,aDBJRvwx&n$rpRGaWxADLVA BbQtiJ\\nXQkzac!;zdGnDct?GGiDHs?uUfECqrmWY!pTRXmr\"\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gw2IDss555iT",
        "colab_type": "code",
        "outputId": "b3b4318c-da73-4f0e-b7ff-6ff03bbf6d97",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "## Loss function\n",
        "def loss(labels, logits):\n",
        "  return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
        "\n",
        "example_batch_loss  = loss(target_example_batch, example_batch_predictions)\n",
        "print(\"예측 배열 크기(shape): \", example_batch_predictions.shape, \" # (배치 크기, 시퀀스 길이, 어휘 사전 크기\")\n",
        "print(\"스칼라 손실:          \", example_batch_loss.numpy().mean())"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "예측 배열 크기(shape):  (64, 100, 65)  # (배치 크기, 시퀀스 길이, 어휘 사전 크기\n",
            "스칼라 손실:           4.174968\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O4pbpjON58vP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(optimizer='adam', loss=loss)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i7KmxCwl5-_u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Callback을 이용한 checkpoint 저장\n",
        "\n",
        "# 체크포인트가 저장될 디렉토리\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "# 체크포인트 파일 이름\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
        "\n",
        "checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_prefix,\n",
        "    save_weights_only=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u3i8ZOSH6Ddg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Epoch 설정\n",
        "EPOCHS=30"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iJk050P-6F2i",
        "colab_type": "code",
        "outputId": "474495d5-d238-40f1-ec78-70d32806c759",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "## Training\n",
        "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            "172/172 [==============================] - 9s 54ms/step - loss: 2.5933\n",
            "Epoch 2/30\n",
            "172/172 [==============================] - 9s 54ms/step - loss: 1.9014\n",
            "Epoch 3/30\n",
            "172/172 [==============================] - 9s 54ms/step - loss: 1.6534\n",
            "Epoch 4/30\n",
            "172/172 [==============================] - 9s 54ms/step - loss: 1.5157\n",
            "Epoch 5/30\n",
            "172/172 [==============================] - 9s 54ms/step - loss: 1.4334\n",
            "Epoch 6/30\n",
            "172/172 [==============================] - 9s 54ms/step - loss: 1.3758\n",
            "Epoch 7/30\n",
            "172/172 [==============================] - 9s 54ms/step - loss: 1.3302\n",
            "Epoch 8/30\n",
            "172/172 [==============================] - 9s 55ms/step - loss: 1.2920\n",
            "Epoch 9/30\n",
            "172/172 [==============================] - 9s 55ms/step - loss: 1.2554\n",
            "Epoch 10/30\n",
            "172/172 [==============================] - 9s 55ms/step - loss: 1.2209\n",
            "Epoch 11/30\n",
            "172/172 [==============================] - 9s 55ms/step - loss: 1.1871\n",
            "Epoch 12/30\n",
            "172/172 [==============================] - 9s 55ms/step - loss: 1.1522\n",
            "Epoch 13/30\n",
            "172/172 [==============================] - 9s 55ms/step - loss: 1.1159\n",
            "Epoch 14/30\n",
            "172/172 [==============================] - 9s 55ms/step - loss: 1.0784\n",
            "Epoch 15/30\n",
            "172/172 [==============================] - 9s 55ms/step - loss: 1.0387\n",
            "Epoch 16/30\n",
            "172/172 [==============================] - 9s 55ms/step - loss: 0.9986\n",
            "Epoch 17/30\n",
            "172/172 [==============================] - 9s 54ms/step - loss: 0.9591\n",
            "Epoch 18/30\n",
            "172/172 [==============================] - 9s 54ms/step - loss: 0.9192\n",
            "Epoch 19/30\n",
            "172/172 [==============================] - 9s 54ms/step - loss: 0.8789\n",
            "Epoch 20/30\n",
            "172/172 [==============================] - 9s 54ms/step - loss: 0.8399\n",
            "Epoch 21/30\n",
            "172/172 [==============================] - 9s 54ms/step - loss: 0.8033\n",
            "Epoch 22/30\n",
            "172/172 [==============================] - 9s 54ms/step - loss: 0.7683\n",
            "Epoch 23/30\n",
            "172/172 [==============================] - 9s 54ms/step - loss: 0.7357\n",
            "Epoch 24/30\n",
            "172/172 [==============================] - 9s 54ms/step - loss: 0.7043\n",
            "Epoch 25/30\n",
            "172/172 [==============================] - 9s 54ms/step - loss: 0.6761\n",
            "Epoch 26/30\n",
            "172/172 [==============================] - 9s 54ms/step - loss: 0.6514\n",
            "Epoch 27/30\n",
            "172/172 [==============================] - 9s 54ms/step - loss: 0.6282\n",
            "Epoch 28/30\n",
            "172/172 [==============================] - 9s 54ms/step - loss: 0.6071\n",
            "Epoch 29/30\n",
            "172/172 [==============================] - 9s 53ms/step - loss: 0.5877\n",
            "Epoch 30/30\n",
            "172/172 [==============================] - 9s 54ms/step - loss: 0.5717\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M8p-k-7l6HZv",
        "colab_type": "code",
        "outputId": "1937a2e0-44a1-424e-dc22-5ed73631e085",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        "## latest checkpoint 확인\n",
        "tf.train.latest_checkpoint(checkpoint_dir)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'./training_checkpoints/ckpt_30'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "69iMBai76K9w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## 새 model 생성 후 weight load\n",
        "model = build_model(vocab_size, embedding_dim, rnn_units, batch_size=1)\n",
        "\n",
        "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
        "\n",
        "model.build(tf.TensorShape([1, None]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FwvYwu3_6MTd",
        "colab_type": "code",
        "outputId": "65ffe78d-c7b0-496e-a45a-d7b29958c5a6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 278
        }
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_1 (Embedding)      (1, None, 256)            16640     \n",
            "_________________________________________________________________\n",
            "lstm_1 (LSTM)                (1, None, 1024)           5246976   \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (1, None, 65)             66625     \n",
            "=================================================================\n",
            "Total params: 5,330,241\n",
            "Trainable params: 5,330,241\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5oWn4PDk6O0t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## 생성 결과 확인\n",
        "\n",
        "def generate_text(model, start_string):\n",
        "  # 평가 단계 (학습된 모델을 사용하여 텍스트 생성)\n",
        "\n",
        "  # 생성할 문자의 수\n",
        "  num_generate = 1000\n",
        "\n",
        "  # 시작 문자열을 숫자로 변환(벡터화)\n",
        "  input_eval = [char2idx[s] for s in start_string]\n",
        "  input_eval = tf.expand_dims(input_eval, 0)\n",
        "  \n",
        "\n",
        "  # 결과를 저장할 빈 문자열\n",
        "  text_generated = []\n",
        "\n",
        "  # 온도가 낮으면 더 예측 가능한 텍스트가 됩니다.\n",
        "  # 온도가 높으면 더 의외의 텍스트가 됩니다.\n",
        "  # 최적의 세팅을 찾기 위한 실험\n",
        "  temperature = 1\n",
        "  \n",
        "  # 여기에서 배치 크기 == 1\n",
        "  #odel.reset_states()\n",
        "  for i in range(num_generate):      \n",
        "      predictions = model(input_eval)\n",
        "      # 배치 차원 제거\n",
        "      predictions = tf.squeeze(predictions, 0)\n",
        "\n",
        "      # 범주형 분포를 사용하여 모델에서 리턴한 단어 예측\n",
        "      predictions = predictions / temperature\n",
        "      predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
        "\n",
        "      # 예측된 단어를 다음 입력으로 모델에 전달\n",
        "      # 이전 은닉 상태와 함께\n",
        "      input_eval = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "      text_generated.append(idx2char[predicted_id])\n",
        "\n",
        "  return (start_string + ''.join(text_generated))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CkQkLT1g6T2R",
        "colab_type": "code",
        "outputId": "f4d626f4-0194-4b0b-a0ec-d1bad3025372",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 745
        }
      },
      "source": [
        "print(generate_text(model, start_string=u\"ROMEO: \"))"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ROMEO: if a gentleman of friendshough,\n",
            "that says the ghe Earth or Warwick's in brother's last!\n",
            "Take this in quiet, gentle Warwick, to be brief when nature still have moder than in that kindly go\n",
            "To put my practise handle wretches wrong;\n",
            "For bound well long his heir uncertainty;\n",
            "For then, if in this rage,\n",
            "To think of heart thou wast born hotted!\n",
            "She will be taken on my father's face,\n",
            "Whom I was fair load inworth that there pregister to my wife;\n",
            "I'll have found ingeft. There's no short.\n",
            "\n",
            "PETRUCHIO:\n",
            "Did me succo;\n",
            "For I into in a mighty vain carried\n",
            "Than my accusation fight so red:\n",
            "But when ere I can fly the danger.\n",
            "\n",
            "Shepherd:\n",
            "Ay, sir.\n",
            "\n",
            "PROSPERO:\n",
            "Thou worthiest in blood,\n",
            "Of no more than a cause to so, and I have counterfeit supposed like mine arms.\n",
            "\n",
            "NORTHUMBERLAND:\n",
            "The head the singlenous to my mistress,--\n",
            "\n",
            "CURTIS:\n",
            "Though you are, prithee, see? VING RICHARD II:\n",
            "With all Polixenes: in these confidence\n",
            "I am for you.\n",
            "\n",
            "MENENIUS:\n",
            "I'll under-to believe\n",
            "In what you shall.\n",
            "\n",
            "VALERIO:\n",
            "None, sir; no,\n",
            "I must\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3bNqqzQd6Whr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}